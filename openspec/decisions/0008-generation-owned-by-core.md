# 0008：模式A——生成（LLM）由核心底座统一负责

## 状态

已决定

## 背景

Coze/Dify 等平台提供“可配置模型”的能力，适合做应用层编排与交付。但本项目希望在工程化层面突出：

- 权限与数据隔离（检索过滤必须可信且可审计）
- 成本与治理（模型版本/参数/调用成本可控）
- 可观测性（链路耗时、失败原因、命中片段可追踪）
- 一致性（同一问题在同一上下文下输出稳定、可复现）

如果让平台侧自行决定最终生成模型并直接产出答案，会导致治理分散与难以保证“检索→生成”的一致性与审计闭环。

## 决策

采用模式 A：**最终答案生成（LLM 调用）由核心底座统一负责**。

- 平台（Coze/Dify）作为上层入口与编排层，只负责触发调用、展示结果、多端发布与运营能力。
- 核心底座负责 RAG 的完整编排：检索（含权限过滤）→ 上下文组装 → 调用 LLM → 输出答案与引用（citations）。

## 具体约束

- 平台侧不得绕过底座直接生成“最终答案”并对用户宣称为系统答案；系统答案必须来自底座 `/api/chat`（或等价接口）。
- 平台侧可以使用自身模型做“非关键辅助文本”，例如：
  - 欢迎语、引导语、运营文案润色
  - UI/工作流节点的非知识性说明
 但不得替代底座的检索增强问答主链路。

## 影响与权衡

优点：

- 技术深度更集中：权限、检索、提示词与模型治理在底座一处体现。
- 治理统一：成本、审计、可观测性与模型版本控制更易落地。
- 平台可替换：Coze/Dify/n8n 仅是入口与编排器，不影响底座语义。

代价：

- 平台“配置模型”对最终答案不生效（只影响平台侧辅助文本）。
- 底座需要承担更多 LLM 相关治理与稳定性工作（后续可通过多 Provider 适配与配置管理缓解）。

## 后续工作

- 在 `openspec/specs/0001-mvp-rag-knowledge-nexus-spec.md` 明确：模式 A 为主路径，平台以调用底座接口获取最终答案与 citations。
- 底座侧 LLM/Embedding 默认 Provider 已决定采用 OpenAI Compatible API（见 `openspec/decisions/0009-llm-default-openai.md`）；仍需定义默认模型参数与审计字段。
